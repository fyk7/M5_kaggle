{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"## new train data\nimport pandas as pd\n\ndf_train_full = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\ndf_train_full.iloc[:, -31:].head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"## importing packages\nimport numpy as np\nimport pandas as pd\nfrom typing import Union\nfrom tqdm.notebook import tqdm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = df_train_full.iloc[:, :-28].copy()\nvalid_df = df_train_full.iloc[:, -28:].copy()\ntrain_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\ntrain_y.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_df.head()#6/1までは1913までのデータしかなかったが、新たに1941まで開示された。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_target_columns = train_y.columns.tolist()\nprint(train_target_columns[-20:])#1900日分のカラム","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['all_id'] = 0\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\nprint(id_columns)#id, item, dept, category, store, state, all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\nprint(valid_target_columns)#検証期間28日分のカラム","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_columns = train_y.iloc[:, -28:].columns.tolist()#重みの導出に用いるカラム\nprint(weight_columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if not all([c in valid_df.columns for c in id_columns]):\n    valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\nvalid_df.head()#valid_dfだけだとd_\\d{4}しかなくて寂しいから、_idを加えてあげる。\n#6/1に公開されたデータを用いている。\n#このvalid_dfの販売数を用いて評価指標の重みを導出する。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"group_ids = (\n        'all_id',\n        'cat_id',\n        'state_id',\n        'dept_id',\n        'store_id',\n        'item_id',\n        ['state_id', 'cat_id'],\n        ['state_id', 'dept_id'],\n        ['store_id', 'cat_id'],\n        ['store_id', 'dept_id'],\n        ['item_id', 'state_id'],\n        ['item_id', 'store_id']\n    )\n\nfor i, group_id in enumerate(tqdm(group_ids)):\n    train_y = train_df.groupby(group_id)[train_target_columns].sum()#検証期間28日分のカラム\n    scale = []\n    counter = 0\n    for _, row in train_y.iterrows():#1900日分のカラム\n        series = row.values[np.argmax(row.values != 0):]#sumの値が0ではない日付以降\n        scale.append(((series[1:] - series[:-1]) ** 2).mean())#(ある日付-前日)**2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_calendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")\ndf_prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\")\nday_to_week = df_calendar.set_index('d')['wm_yr_wk']\nday_to_week.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_df = df_train_full[['item_id', 'store_id'] + weight_columns].set_index(['item_id', 'store_id'])\nweight_df.head()#item_id, store_idをindexにした","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#横持ちのマトリックスを縦持ちに変更する。\nweight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\nweight_df.head(20)#valueは販売数","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\nweight_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_df = weight_df.merge(df_prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\nweight_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_df['value'] = weight_df['value'] * weight_df['sell_price']\nweight_df.head()\n#valueは販売数*販売価格","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#今度は逆に縦持ちから横持ちへと変換。\nweight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\nweight_df.head()\n#valueは購入総額\n#unstack(level=2)とすると、indexの左から二つまでが新たなindexとなり、右の一つがvalueとなる。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_df = weight_df.loc[zip(train_df.item_id, train_df.store_id), :].reset_index(drop=True)\nweight_df.head()\n#valueだけ取り出す。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weight_df = pd.concat([train_df[id_columns], weight_df], axis=1, sort=False)\nweight_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)#縦に足してから、横に足す。\nlv_weight.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lv_weight.sum()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"## evaluation metric\n## from https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834 and edited to get scores at all levels\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()#1914~\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)#1914~\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'cat_id',\n            'state_id',\n            'dept_id',\n            'store_id',\n            'item_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            #scaleの導出には1900日全てを使用する。\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()#各日にちのvalueを縦方向に足し合わせる\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]#例えば3個のstateの各日にちごとの売り上げ数の総和\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())#前日との差の二乗平均がscaleになる\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))#前日とのmean_sequre_error, groupbyの項目数だけ存在する。\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())#28日分のid\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)#縦に足してから、横に足している\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())#lv_weightを正規化する。#縦に足してから、横に足して、それをさらに足して総和\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})#item, store, dayで売り上げ個数を集計\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)#週カラムを新たに追加(集計はしていない)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])#item, store, weekごとのprice\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']#valueはitem, store, dayごとの各商品の販売総額\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']#横持ちに戻す。valueは商品総額\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)#indexを削除する\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)#元のidカラムたちと結合して\n        return weight_df#各日にちの販売総額がvalueとして格納されている。\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')#ここで1941のカラムを使用しているから、validの値は計算に必要である。\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)#二乗誤差平均#groupbyで集計されたものから,valid_preds(しっかりとgroupbyされている)を引く\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]):\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        group_ids = []\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            #rmsseの計算にはweightはいらないが、validの値が必要である。\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)#あらかじめvalid_predsをgroupbyしてくれている。\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)#prodでさりげなく重みを掛け合わせている。\n            group_ids.append(group_id)\n            all_scores.append(lv_scores.sum())\n\n        return group_ids, all_scores\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## public LB rank\ndef get_lb_rank(score):\n    df_lb = pd.read_csv(\"../input/m5-accuracy-final-public-lb/m5-forecasting-accuracy-publicleaderboard-rank.csv\")\n    return (df_lb.Score <= score).sum() + 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## reading data\ndf_calendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")\ndf_prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\")\ndf_sample_submission = pd.read_csv(\"../input/m5-forecasting-accuracy/sample_submission.csv\")\ndf_sample_submission[\"order\"] = range(df_sample_submission.shape[0])\n\ndf_train = df_train_full.iloc[:, :-28]\ndf_valid = df_train_full.iloc[:, -28:]\nevaluator = WRMSSEEvaluator(df_train, df_valid, df_calendar, df_prices)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluator.lv5_scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## structure of validation data\npreds_valid = df_valid.copy() + np.random.randint(100, size = df_valid.shape)\npreds_valid.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## evaluating random submission\ngroups, scores = evaluator.score(preds_valid)\n\nscore_public_lb = np.mean(scores)\nscore_public_rank = get_lb_rank(score_public_lb)\n\nfor i in range(len(groups)):\n    print(f\"Score for group {groups[i]}: {round(scores[i], 5)}\")\n\nprint(f\"\\nPublic LB Score: {round(score_public_lb, 5)}\")\nprint(f\"Public LB Rank: {score_public_rank}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_valid = pd.read_csv(\"../input/m5-three-shades-of-dark-darker-magic/submission_v1.csv\")\npreds_valid = preds_valid[preds_valid.id.str.contains(\"validation\")]\npreds_valid.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_sample_submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds_valid = preds_valid.merge(df_sample_submission[[\"id\", \"order\"]], on = \"id\").sort_values(\"order\")#.drop([\"id\", \"order\"], axis = 1).reset_index(drop = True)\n#df_sample_submissionはid, order列のみしか使用していない。\npreds_valid.head()\npreds_valid.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#submissionファイルを読み込んで、validationという文字をidに含んでいる行のみを抽出して、カラムをrename\npreds_valid = preds_valid.merge(df_sample_submission[[\"id\", \"order\"]], on = \"id\").sort_values(\"order\").drop([\"id\", \"order\"], axis = 1).reset_index(drop = True)\npreds_valid.rename(columns = {\n    \"F1\": \"d_1914\", \"F2\": \"d_1915\", \"F3\": \"d_1916\", \"F4\": \"d_1917\", \"F5\": \"d_1918\", \"F6\": \"d_1919\", \"F7\": \"d_1920\",\n    \"F8\": \"d_1921\", \"F9\": \"d_1922\", \"F10\": \"d_1923\", \"F11\": \"d_1924\", \"F12\": \"d_1925\", \"F13\": \"d_1926\", \"F14\": \"d_1927\",\n    \"F15\": \"d_1928\", \"F16\": \"d_1929\", \"F17\": \"d_1930\", \"F18\": \"d_1931\", \"F19\": \"d_1932\", \"F20\": \"d_1933\", \"F21\": \"d_1934\",\n    \"F22\": \"d_1935\", \"F23\": \"d_1936\", \"F24\": \"d_1937\", \"F25\": \"d_1938\", \"F26\": \"d_1939\", \"F27\": \"d_1940\", \"F28\": \"d_1941\"\n}, inplace = True)\n\ngroups, scores = evaluator.score(preds_valid)\n\nscore_public_lb = np.mean(scores)\nscore_public_rank = get_lb_rank(score_public_lb)\n\nfor i in range(len(groups)):\n    print(f\"Score for group {groups[i]}: {round(scores[i], 5)}\")\n\nprint(f\"\\nPublic LB Score: {round(score_public_lb, 5)}\")\nprint(f\"Public LB Rank: {score_public_rank}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## evaluating submission from public kernel M5 - Witch Time\n## from https://www.kaggle.com/kyakovlev/m5-witch-time\npreds_valid = pd.read_csv(\"../input/m5-witch-time/submission.csv\")\npreds_valid = preds_valid[preds_valid.id.str.contains(\"validation\")]\npreds_valid = preds_valid.merge(df_sample_submission[[\"id\", \"order\"]], on = \"id\").sort_values(\"order\").drop([\"id\", \"order\"], axis = 1).reset_index(drop = True)\npreds_valid.rename(columns = {\n    \"F1\": \"d_1914\", \"F2\": \"d_1915\", \"F3\": \"d_1916\", \"F4\": \"d_1917\", \"F5\": \"d_1918\", \"F6\": \"d_1919\", \"F7\": \"d_1920\",\n    \"F8\": \"d_1921\", \"F9\": \"d_1922\", \"F10\": \"d_1923\", \"F11\": \"d_1924\", \"F12\": \"d_1925\", \"F13\": \"d_1926\", \"F14\": \"d_1927\",\n    \"F15\": \"d_1928\", \"F16\": \"d_1929\", \"F17\": \"d_1930\", \"F18\": \"d_1931\", \"F19\": \"d_1932\", \"F20\": \"d_1933\", \"F21\": \"d_1934\",\n    \"F22\": \"d_1935\", \"F23\": \"d_1936\", \"F24\": \"d_1937\", \"F25\": \"d_1938\", \"F26\": \"d_1939\", \"F27\": \"d_1940\", \"F28\": \"d_1941\"\n}, inplace = True)\n\ngroups, scores = evaluator.score(preds_valid)#valid_dfを上記のように処理したのち、score関数に渡せば、lbを導出してくれる。\n\nscore_public_lb = np.mean(scores)\nscore_public_rank = get_lb_rank(score_public_lb)\n\nfor i in range(len(groups)):\n    print(f\"Score for group {groups[i]}: {round(scores[i], 5)}\")\n\nprint(f\"\\nPublic LB Score: {round(score_public_lb, 5)}\")\nprint(f\"Public LB Rank: {score_public_rank}\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train_full = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\ndf_calendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")\ndf_prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\")\ndf_sample_submission = pd.read_csv(\"../input/m5-forecasting-accuracy/sample_submission.csv\")\ndf_sample_submission[\"order\"] = range(df_sample_submission.shape[0])\n\ndf_train = df_train_full.iloc[:, :-28]\ndf_valid = df_train_full.iloc[:, -28:]\nevaluator = WRMSSEEvaluator(df_train, df_valid, df_calendar, df_prices)#df_validは販売数も判明していないと計算できない。","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_lb_rank(score):\n    df_lb = pd.read_csv(\"../input/m5-accuracy-final-public-lb/m5-forecasting-accuracy-publicleaderboard-rank.csv\")\n    return (df_lb.Score <= score).sum() + 1\n\ndef calc_lb(preds_path, evaluator):\n    preds_valid = pd.read_csv(preds_path)\n    preds_valid = preds_valid[preds_valid.id.str.contains(\"validation\")]\n    preds_valid = preds_valid.merge(df_sample_submission[[\"id\", \"order\"]], on = \"id\").sort_values(\"order\").drop([\"id\", \"order\"], axis = 1).reset_index(drop = True)\n    preds_valid.rename(columns = {\n        \"F1\": \"d_1914\", \"F2\": \"d_1915\", \"F3\": \"d_1916\", \"F4\": \"d_1917\", \"F5\": \"d_1918\", \"F6\": \"d_1919\", \"F7\": \"d_1920\",\n        \"F8\": \"d_1921\", \"F9\": \"d_1922\", \"F10\": \"d_1923\", \"F11\": \"d_1924\", \"F12\": \"d_1925\", \"F13\": \"d_1926\", \"F14\": \"d_1927\",\n        \"F15\": \"d_1928\", \"F16\": \"d_1929\", \"F17\": \"d_1930\", \"F18\": \"d_1931\", \"F19\": \"d_1932\", \"F20\": \"d_1933\", \"F21\": \"d_1934\",\n        \"F22\": \"d_1935\", \"F23\": \"d_1936\", \"F24\": \"d_1937\", \"F25\": \"d_1938\", \"F26\": \"d_1939\", \"F27\": \"d_1940\", \"F28\": \"d_1941\"\n    }, inplace = True)\n\n    groups, scores = evaluator.score(preds_valid)#valid_dfを上記のように処理したのち、score関数に渡せば、lbを導出してくれる。\n\n    score_public_lb = np.mean(scores)\n    score_public_rank = get_lb_rank(score_public_lb)\n    \n    return score_public_lb, score_public_rank","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#３ヶ月分のlbの平均値が高いモデルを提出する?\n#同月前年の重みを使用する?\n#上の重みをNNに組み込むためにはどうすればいいのか?\n#実測値から重みを計算->valid_predsに適用(実際の28日分の重みが判明しない限りは、scale, weightは計算できない)\n#(先月の売り上げからは計算できない。)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}